# ================= Super-Hybrid Ensemble for Session Classification (Full with CNN-D + MetaMLP) =================
# Requires: numpy,pandas,scikit-learn,lightgbm,torch,matplotlib,seaborn
# Run in Jupyter. Comments/explanations in English.

import os
import math
import random
import warnings
from datetime import timedelta

import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.linear_model import RidgeClassifierCV
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

warnings.filterwarnings('ignore')
np.random.seed(42)
random.seed(42)
if torch.cuda.is_available():
    torch.manual_seed(42)

# ------------------------------
# PARAMETERS
# ------------------------------
LOOKBACK = 10
BATCH_SIZE = 64
EPOCHS_CNN = 35
LR_DL = 1e-3
NUM_CLASSES = 3
RANDOM_STATE = 42

# ------------------------------
# 1. Load data
# ------------------------------
file_path = "/home/karim/Downloads/GOLDFILES/gold.csv"
df = pd.read_csv(file_path)
df['Gmt time'] = pd.to_datetime(df['Gmt time'])
df.drop(columns=['Unnamed: 0', 'Volume'], inplace=True, errors='ignore')
df['Date'] = df['Gmt time'].dt.date
df['hour'] = df['Gmt time'].dt.hour

def add_trading_sessions_if_missing(df):
    if 'Session_NY' in df.columns:
        return df
    utc = df['Gmt time'].dt.tz_localize('UTC')
    df['Session_NY'] = utc.dt.tz_convert('America/New_York').dt.hour.between(8, 16).astype(int)
    df['Session_London'] = utc.dt.tz_convert('Europe/London').dt.hour.between(8, 16).astype(int)
    df['Session_Tokyo'] = utc.dt.tz_convert('Asia/Tokyo').dt.hour.between(9, 17).astype(int)
    df['Session_Sydney'] = utc.dt.tz_convert('Australia/Sydney').dt.hour.between(7, 15).astype(int)
    return df

df = add_trading_sessions_if_missing(df)

# ------------------------------
# 2. Build daily session-level DataFrames
# ------------------------------
def build_session_daily(df, session):
    mask = df[f"Session_{session}"] == 1
    temp = df[mask].sort_values('Gmt time').copy()
    if temp.empty:
        return pd.DataFrame()
    daily = temp.groupby('Date').agg(
        Open=('Open','first'),
        High=('High','max'),
        Low=('Low','min'),
        Close=('Close','last'),
        StartTime=('Gmt time','first')
    ).reset_index()
    daily[f'{session}_ret'] = (daily['Close'] - daily['Open']) / (daily['Open'] + 1e-9)
    daily[f'{session}_range'] = (daily['High'] - daily['Low']) / (daily['Open'] + 1e-9)
    daily[f'{session}_body'] = (daily['Close'] - daily['Open']) / (daily['High'] - daily['Low'] + 1e-9)
    daily[f'{session}_dir_strength'] = np.abs(daily[f'{session}_ret']) / (daily[f'{session}_range'] + 1e-9)
    daily[f'{session}_volatility'] = (daily['High'] - daily['Low']) / (daily['Close'] + 1e-9)
    daily[f'Prev_{session}_ret_1'] = daily[f'{session}_ret'].shift(1)
    daily[f'Prev_{session}_ret_2'] = daily[f'{session}_ret'].shift(2)
    daily[f'{session}_ret_ma3'] = daily[f'{session}_ret'].rolling(3, min_periods=1).mean()
    daily[f'{session}_vol_ma3'] = daily[f'{session}_volatility'].rolling(3, min_periods=1).mean()
    daily.fillna(0, inplace=True)
    return daily

SESSIONS = ['NY','London','Tokyo','Sydney']
session_daily = {s: build_session_daily(df,s) for s in SESSIONS}

# ------------------------------
# 3. Build Directional Targets
# ------------------------------
def build_structure_target_per_day(session, high_q=0.7, low_q=0.3):
    daily = session_daily[session].copy()
    if daily.empty: return daily
    pos = (daily['Close'] - daily['Low']) / (daily['High'] - daily['Low'] + 1e-9)
    daily['target'] = pos.apply(lambda x: 2 if x>high_q else 0 if x<low_q else 1)
    return daily[['Date','StartTime','target']]

targets_daily = {s: build_structure_target_per_day(s) for s in SESSIONS}

# ------------------------------
# 4. Prepare samples
# ------------------------------
def make_samples_for_session(session, lookback=LOOKBACK):
    df_daily = session_daily[session].copy()
    if df_daily.empty: return None
    targ = targets_daily[session].set_index('Date')['target']
    seq_features = [f'{session}_ret', f'{session}_range', f'{session}_body',
                    f'{session}_dir_strength', f'{session}_volatility']
    tab_features = [f'Prev_{session}_ret_1', f'Prev_{session}_ret_2', f'{session}_ret_ma3', f'{session}_vol_ma3']
    X_seq, X_tab, y, dates = [], [], [], []
    for idx in range(lookback, len(df_daily)):
        window = df_daily.iloc[idx-lookback:idx]
        cur_date = df_daily.iloc[idx]['Date']
        if cur_date not in targ.index: continue
        seq = window[seq_features].values
        tab = df_daily.iloc[idx][tab_features].values.astype(float)
        X_seq.append(seq)
        X_tab.append(tab)
        y.append(int(targ.loc[cur_date]))
        dates.append(cur_date)
    return {'X_seq': np.array(X_seq), 'X_tab': np.array(X_tab),
            'y': np.array(y), 'dates': dates, 'seq_features': seq_features, 'tab_features': tab_features}

samples = {s: make_samples_for_session(s) for s in SESSIONS}

# ------------------------------
# 5. Dataset class
# ------------------------------
class SeqDataset(Dataset):
    def __init__(self, X, y): self.X, self.y = torch.tensor(X,dtype=torch.float32), torch.tensor(y,dtype=torch.long)
    def __len__(self): return len(self.y)
    def __getitem__(self, idx): return self.X[idx], self.y[idx]

# ------------------------------
# 6. CNN3Head
# ------------------------------
class CNN3Head(nn.Module):
    def __init__(self, in_features, d_model=64, num_classes=NUM_CLASSES):
        super().__init__()
        self.branch1 = nn.Sequential(nn.Conv1d(in_features,32,2), nn.ReLU(), nn.AdaptiveAvgPool1d(1))
        self.branch2 = nn.Sequential(nn.Conv1d(in_features,32,3), nn.ReLU(), nn.AdaptiveAvgPool1d(1))
        self.branch3 = nn.Sequential(nn.Conv1d(in_features,32,5), nn.ReLU(), nn.AdaptiveAvgPool1d(1))
        self.fc = nn.Sequential(nn.Linear(32*3,d_model), nn.ReLU(), nn.Dropout(0.2), nn.Linear(d_model,num_classes))
    def forward(self, x):
        x = x.permute(0,2,1)
        L = x.size(2)
        if L<5: x = nn.functional.pad(x,(0,5-L))
        b1,b2,b3 = [b(x).squeeze(-1) for b in [self.branch1,self.branch2,self.branch3]]
        return self.fc(torch.cat([b1,b2,b3],dim=1))

# ------------------------------
# 7. Transformer Encoder
# ------------------------------
class PositionalEncoding(nn.Module):
    def __init__(self,d_model,max_len=500):
        super().__init__()
        pe = torch.zeros(max_len,d_model)
        pos = torch.arange(0,max_len,dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0,d_model,2).float()*(-math.log(10000.0)/d_model))
        pe[:,0::2]=torch.sin(pos*div_term); pe[:,1::2]=torch.cos(pos*div_term)
        self.register_buffer('pe',pe.unsqueeze(0))
    def forward(self,x): return x+self.pe[:,:x.size(1),:]

class TransformerEncoderModel(nn.Module):
    def __init__(self,in_features,d_model=64,nhead=4,num_layers=2,num_classes=NUM_CLASSES,dropout=0.1):
        super().__init__()
        self.input_proj = nn.Linear(in_features,d_model)
        self.pos_enc = PositionalEncoding(d_model)
        layer = nn.TransformerEncoderLayer(d_model,nhead,dim_feedforward=d_model*2,dropout=dropout)
        self.transformer = nn.TransformerEncoder(layer,num_layers=num_layers)
        self.pool = nn.AdaptiveAvgPool1d(1)
        self.out = nn.Sequential(nn.Linear(d_model,d_model//2), nn.ReLU(), nn.Dropout(dropout), nn.Linear(d_model//2,num_classes))
    def forward(self,x):
        x = self.input_proj(x); x=self.pos_enc(x)
        x_t = x.permute(1,0,2); encoded=self.transformer(x_t)
        pooled = self.pool(encoded.permute(1,2,0)).squeeze(-1)
        return self.out(pooled)

# ------------------------------
# 8. CNN Short Pattern Expert
# ------------------------------
class CNN_ShortPattern(nn.Module):
    def __init__(self,in_features,hidden_dim=64,num_classes=NUM_CLASSES):
        super().__init__()
        self.conv1 = nn.Conv1d(in_features,hidden_dim,2)
        self.relu = nn.ReLU()
        self.pool = nn.AdaptiveAvgPool1d(1)
        self.fc = nn.Linear(hidden_dim,num_classes)
    def forward(self,x):
        x = x.permute(0,2,1)
        x = self.pool(self.relu(self.conv1(x))).squeeze(-1)
        return self.fc(x)

# ------------------------------
# 9. Meta MLP
# ------------------------------
class MetaMLP(nn.Module):
    def __init__(self,input_dim,hidden_dim=64,num_classes=NUM_CLASSES,dropout=0.2):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim,hidden_dim),
            nn.ReLU(), nn.Dropout(dropout),
            nn.Linear(hidden_dim,hidden_dim//2),
            nn.ReLU(), nn.Dropout(dropout),
            nn.Linear(hidden_dim//2,num_classes)
        )
    def forward(self,x): return self.net(x)

# ------------------------------
# 10. Training Helper
# ------------------------------
def train_dl_model(model,train_loader,val_loader,epochs=EPOCHS_CNN,lr=LR_DL,device='cpu'):
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    model.to(device)
    best_acc=0.0; best_state=None
    history={'train_loss':[],'val_loss':[]}
    for ep in range(epochs):
        model.train()
        total_loss=0
        for Xb,yb in train_loader:
            Xb,yb = Xb.to(device), yb.to(device)
            optimizer.zero_grad()
            loss = criterion(model(Xb), yb)
            loss.backward(); optimizer.step()
            total_loss += loss.item()*Xb.size(0)
        train_loss = total_loss/len(train_loader.dataset)
        model.eval()
        total_vloss=0; preds=[]; trues=[]
        with torch.no_grad():
            for Xv,yv in val_loader:
                Xv,yv = Xv.to(device), yv.to(device)
                out = model(Xv); lossv=criterion(out,yv)
                total_vloss+=lossv.item()*Xv.size(0)
                preds.append(torch.argmax(out,dim=1).cpu().numpy())
                trues.append(yv.cpu().numpy())
        val_loss = total_vloss/len(val_loader.dataset)
        preds = np.concatenate(preds); trues = np.concatenate(trues)
        val_acc = (preds==trues).mean()
        history['train_loss'].append(train_loss); history['val_loss'].append(val_loss)
        if val_acc>best_acc: best_acc=val_acc; best_state=model.state_dict()
        print(f"Epoch {ep+1}/{epochs} | train_loss {train_loss:.4f} | val_loss {val_loss:.4f} | val_acc {val_acc:.4f}")
    if best_state is not None: model.load_state_dict(best_state)
    return model, history, best_acc

def get_probs_dl(model,X_np,device='cpu',batch_size=256):
    model.eval(); probs=[]
    with torch.no_grad():
        for i in range(0,len(X_np),batch_size):
            chunk = X_np[i:i+batch_size]; t=torch.tensor(chunk,dtype=torch.float32).to(device)
            out = model(t); probs.append(nn.functional.softmax(out,dim=1).cpu().numpy())
    return np.vstack(probs) if len(probs)>0 else np.zeros((len(X_np),NUM_CLASSES))

def to_probs_from_decision(dec):
    if dec.ndim==1: dec=np.vstack([-dec,dec]).T
    ex=np.exp(dec-dec.max(axis=1,keepdims=True))
    return ex/ex.sum(axis=1,keepdims=True)

# ------------------------------
# 11. Main Loop
# ------------------------------
results_summary={}
device='cuda' if torch.cuda.is_available() else 'cpu'

for s in SESSIONS:
    samp = samples[s]; 
    if samp is None: continue
    X_seq,X_tab,y = samp['X_seq'],samp['X_tab'],samp['y']
    n=len(y); idx_train=int(0.8*n)
    X_seq_train,X_seq_test=X_seq[:idx_train],X_seq[idx_train:]
    X_tab_train,X_tab_test=X_tab[:idx_train],X_tab[idx_train:]
    y_train,y_test=y[:idx_train],y[idx_train:]
    print(f"\n=== Session {s} | samples {n} | train {len(y_train)} test {len(y_test)} ===")

    # Expert A: Ridge
    scaler_tab = StandardScaler().fit(X_tab_train)
    X_tab_train_s = scaler_tab.transform(X_tab_train)
    X_tab_test_s = scaler_tab.transform(X_tab_test)
    ridge = RidgeClassifierCV(alphas=np.logspace(-6,6,13), cv=3).fit(X_tab_train_s,y_train)
    probs_a_train = to_probs_from_decision(ridge.decision_function(X_tab_train_s))
    probs_a_test = to_probs_from_decision(ridge.decision_function(X_tab_test_s))
    acc_a = accuracy_score(y_test,np.argmax(probs_a_test,axis=1))
    print(f"Expert A (Ridge) test acc: {acc_a:.4f}")

    # Expert B: CNN3Head
    ds_train=SeqDataset(X_seq_train,y_train); ds_test=SeqDataset(X_seq_test,y_test)
    dl_train=DataLoader(ds_train,batch_size=min(BATCH_SIZE,len(ds_train)),shuffle=True)
    dl_val=DataLoader(ds_test,batch_size=min(BATCH_SIZE,len(ds_test)),shuffle=False)
    in_features=X_seq.shape[2]
    cnn=CNN3Head(in_features=in_features); cnn,hist_cnn,best_acc_cnn=train_dl_model(cnn,dl_train,dl_val,device=device)
    probs_b_train=get_probs_dl(cnn,X_seq_train,device=device)
    probs_b_test=get_probs_dl(cnn,X_seq_test,device=device)
    plt.figure(); plt.plot(hist_cnn['train_loss'],label='train'); plt.plot(hist_cnn['val_loss'],label='val'); plt.title(f'{s} CNN3Head Loss'); plt.legend(); plt.show()

    # Expert C: Transformer
    trans=TransformerEncoderModel(in_features=in_features)
    trans,hist_trans,best_acc_trans=train_dl_model(trans,dl_train,dl_val,device=device)
    probs_c_train=get_probs_dl(trans,X_seq_train,device=device)
    probs_c_test=get_probs_dl(trans,X_seq_test,device=device)
    plt.figure(); plt.plot(hist_trans['train_loss'],label='train'); plt.plot(hist_trans['val_loss'],label='val'); plt.title(f'{s} Transformer Loss'); plt.legend(); plt.show()

    # Expert D: CNN ShortPattern
    dcnn=CNN_ShortPattern(in_features=in_features)
    dcnn,hist_dcnn,best_acc_dcnn=train_dl_model(dcnn,dl_train,dl_val,device=device)
    probs_d_train=get_probs_dl(dcnn,X_seq_train,device=device)
    probs_d_test=get_probs_dl(dcnn,X_seq_test,device=device)
    plt.figure(); plt.plot(hist_dcnn['train_loss'],label='train'); plt.plot(hist_dcnn['val_loss'],label='val'); plt.title(f'{s} CNN ShortPattern Loss'); plt.legend(); plt.show()

    # Meta Features
    meta_train=np.hstack([X_tab_train_s, probs_a_train, probs_b_train, probs_c_train, probs_d_train])
    meta_test=np.hstack([X_tab_test_s, probs_a_test, probs_b_test, probs_c_test, probs_d_test])

    # Clean
    tr_df=pd.DataFrame(meta_train); tr_df['target']=y_train; tr_df.dropna(inplace=True)
    meta_train_clean=tr_df.drop(columns=['target']).values; y_train_clean=tr_df['target'].values.astype(int)
    te_df=pd.DataFrame(meta_test); te_df['target']=y_test; te_df.dropna(inplace=True)
    meta_test_clean=te_df.drop(columns=['target']).values; y_test_clean=te_df['target'].values.astype(int)

    # Meta MLP
    meta_model=MetaMLP(input_dim=meta_train_clean.shape[1])
    ds_meta_train=SeqDataset(meta_train_clean,y_train_clean)
    ds_meta_test=SeqDataset(meta_test_clean,y_test_clean)
    dl_meta_train=DataLoader(ds_meta_train,batch_size=min(BATCH_SIZE,len(ds_meta_train)),shuffle=True)
    dl_meta_val=DataLoader(ds_meta_test,batch_size=min(BATCH_SIZE,len(ds_meta_test)),shuffle=False)
    meta_model,hist_meta,best_meta_acc=train_dl_model(meta_model,dl_meta_train,dl_meta_val,device=device)
    meta_probs_test=get_probs_dl(meta_model,meta_test_clean,device=device)
    meta_preds_test=np.argmax(meta_probs_test,axis=1)
    acc_meta=accuracy_score(y_test_clean,meta_preds_test)
    print(f"Meta MLP test acc: {acc_meta:.4f}")
    plt.figure(); plt.plot(hist_meta['train_loss'],label='train'); plt.plot(hist_meta['val_loss'],label='val'); plt.title(f'{s} MetaMLP Loss'); plt.legend(); plt.show()

    # Save results
    results_summary[s]={'acc_A':acc_a,'acc_B':best_acc_cnn,'acc_C':best_acc_trans,'acc_D':best_acc_dcnn,'acc_meta':acc_meta,
                        'model_ridge':ridge,'model_cnn':cnn,'model_trans':trans,'model_dcnn':dcnn,'model_meta':meta_model,
                        'scaler_tab':scaler_tab,'probs_test':{'a':probs_a_test,'b':probs_b_test,'c':probs_c_test,'d':probs_d_test,'meta':meta_probs_test}}

print("\n=== Summary Across Sessions ===")
for k,v in results_summary.items():
    print(k, "Meta MLP Acc:", v['acc_meta'])
